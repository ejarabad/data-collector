# Proyecto ETL: Colector y Procesador de Datos de Redes Sociales (Simulado)

Este proyecto demuestra un pipeline bÃ¡sico de ExtracciÃ³n, TransformaciÃ³n y Carga (ETL) utilizando Python. Simula la recolecciÃ³n de datos de publicaciones de redes sociales, su limpieza y transformaciÃ³n, y finalmente su carga en una base de datos SQLite para su posterior anÃ¡lisis.

## ğŸš€ Habilidades Demostradas

* **IngenierÃ­a de Datos:** DiseÃ±o e implementaciÃ³n de un pipeline ETL.
* **Python:** Uso de librerÃ­as clave para manipulaciÃ³n de datos y operaciones de sistema.
* **ExtracciÃ³n (Extract):** ObtenciÃ³n de datos de fuentes externas (simulado vÃ­a API REST).
* **TransformaciÃ³n (Transform):** Limpieza, normalizaciÃ³n, renombrado de columnas y creaciÃ³n de nuevas caracterÃ­sticas (Feature Engineering).
* **Carga (Load):** Persistencia de datos limpios en una base de datos relacional (SQLite).
* **Manejo de Archivos:** Lectura y escritura de formatos CSV y JSON.
* **Bases de Datos:** InteracciÃ³n bÃ¡sica con bases de datos SQL.

## ğŸ› ï¸ TecnologÃ­as Utilizadas

* **Python 3.x**
* **Pandas:** Para manipulaciÃ³n y anÃ¡lisis de datos.
* **Requests:** Para realizar peticiones HTTP (extracciÃ³n de datos).
* **SQLite3:** Base de datos ligera y embebida para la capa de carga.
* **Git & GitHub:** Control de versiones y alojamiento del repositorio.

## ğŸ“‚ Estructura del Proyecto
data_collector/
â”œâ”€â”€ collect_data.py       # Script para la ExtracciÃ³n de datos crudos.
â”œâ”€â”€ transform_data.py     # Script para la TransformaciÃ³n y limpieza de datos.
â”œâ”€â”€ load_data.py          # Script para la Carga de datos limpios a la base de datos.
â”œâ”€â”€ requirements.txt      # Lista de dependencias de Python del proyecto.
â”œâ”€â”€ .gitignore            # Archivo para ignorar archivos y carpetas en Git.
â”œâ”€â”€ raw_data/             # Contiene los datos extraÃ­dos en su formato original.
â”‚   â””â”€â”€ social_media__data.csv
â”‚   â””â”€â”€ social_media_data.json
â”œâ”€â”€ cleaned_data/         # Contiene los datos despuÃ©s de la fase de transformaciÃ³n.
â”‚   â””â”€â”€ clean_social_media_data.csv
â””â”€â”€ database/             # Contiene el archivo de la base de datos SQLite.
â””â”€â”€ social_media_data.db

## âš™ï¸ CÃ³mo Ejecutar el Proyecto

Sigue estos pasos para ejecutar el pipeline ETL completo en tu mÃ¡quina local:

1.  **Clonar el repositorio:**
    ```bash
    git clone [https://github.com/TuUsuario/data_engineer_project_jr.git](https://github.com/TuUsuario/data_engineer_project_jr.git)
    cd data_engineer_project_jr
    ```
    *(Nota: Reemplaza `TuUsuario` y `data_engineer_project_jr` con tu informaciÃ³n real de GitHub despuÃ©s de crear el repositorio.)*

2.  **Crear y activar el entorno virtual:**
    ```bash
    python -m venv venv
    # En Windows:
    .\venv\Scripts\activate
    # En macOS/Linux:
    source venv/bin/activate
    ```

3.  **Instalar las dependencias:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Ejecutar el pipeline ETL en secuencia:**
    ```bash
    python collect_data.py
    python transform_data.py
    python load_data.py
    ```
    *(Opcional: Puedes ejecutar cada script individualmente para ver los resultados de cada fase.)*

## ğŸ“ˆ Resultados Esperados

DespuÃ©s de ejecutar los scripts, se generarÃ¡n las siguientes carpetas y archivos:

* `raw_data/`: ContendrÃ¡ `social_media_data.csv` y `social_media_data.json` con los datos tal como se extrajeron.
* `cleaned_data/`: ContendrÃ¡ `clean_social_media_data.csv` con los datos limpios, renombrados y con la nueva columna `content_length`.
* `database/`: ContendrÃ¡ `social_media_data.db`, el archivo de la base de datos SQLite con la tabla `posts` cargada con los datos limpios.

## ğŸ¤ Contribuciones

Â¡Las contribuciones son bienvenidas! Si tienes sugerencias o mejoras, no dudes en abrir un *issue* o enviar un *pull request*.
